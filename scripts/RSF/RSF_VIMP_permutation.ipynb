{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31b7600b",
   "metadata": {},
   "source": [
    "# RSF - VIMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cc77f8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# SET UP\n",
    "################################################################################\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import pickle\n",
    "from collections import Counter\n",
    "#from sksurv.util import Surv\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Set working directory\n",
    "os.chdir(os.path.expanduser(\"~/PhD_Workspace/PredictRecurrence/\"))\n",
    "\n",
    "# Import custom RSF functions\n",
    "sys.path.append(\"/Users/le7524ho/PhD_Workspace/PredictRecurrence/src/\")\n",
    "from src.utils import (\n",
    "    log,\n",
    "    load_training_data,\n",
    "    beta2m,\n",
    "    subset_methylation)\n",
    "from src.annotation_functions import (\n",
    "    run_univariate_cox_for_cpgs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "267d614a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "################################################################################\n",
    "# PARAMS\n",
    "################################################################################\n",
    "\n",
    "# Output directory and files\n",
    "output_dir = \"output/RSF/VIMP_analysis/\" # ⚠️ ADAPT\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "#outfile_univariate_cox = os.path.join(output_dir, \"testset_univariate_cox.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0c9c5981",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# INPUT FILES\n",
    "################################################################################\n",
    "\n",
    "infile_map = {\n",
    "    \"ERpHER2n_Clinical\" : \"./output/RSF/ERpHER2n/Clinical/None/outer_cv_models.pkl\", \n",
    "    \"ERpHER2n_Combined\" : \"./output/RSF/ERpHER2n/Combined/Unadjusted/outer_cv_models.pkl\",\n",
    "    \"ERpHER2n_Methylation\" : \"./output/RSF/ERpHER2n/Methylation/Unadjusted/outer_cv_models.pkl\",\n",
    "\n",
    "    \"TNBC_Clinical\" : \"./output/RSF/TNBC/Clinical/None/outer_cv_models.pkl\",\n",
    "    \"TNBC_Combined\" : \"./output/RSF/TNBC/Combined/Unadjusted/outer_cv_models.pkl\", \n",
    "    \"TNBC_Methylation\" : \"./output/RSF/TNBC/Methylation/Unadjusted/outer_cv_models.pkl\", \n",
    "\n",
    "    \"All_Clinical\" : \"./output/RSF/All/Clinical/None/outer_cv_models.pkl\", \n",
    "    \"All_Combined\" : \"./output/RSF/All/Combined/Unadjusted/outer_cv_models.pkl\", \n",
    "    \"All_Methylation\" : \"./output/RSF/All/Methylation/Unadjusted/outer_cv_models.pkl\" \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c8ae0a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input files\n",
    "infile_betavalues = \"./data/train/train_methylation_unadjusted.csv\" # ⚠️ ADAPT\n",
    "infile_clinical = \"./data/train/train_clinical.csv\"\n",
    "infile_train_ids = \"./data/train/train_subcohorts/ERpHER2n_train_ids.csv\" # sample ids of training cohort"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52777b1",
   "metadata": {},
   "source": [
    "## Load Models and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4b3fb5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "outerfolds = {}\n",
    "for key, filepath in infile_map.items():\n",
    "    outerfolds[key] = joblib.load(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "791e6156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training data.\n",
      "Successfully loaded 205799 CpG IDs for pre-filtering.\n",
      "Warning: 12553 CpGs from the input file are not in the training data.\n",
      "Successfully subsetted methylation data to 193246 pre-filtered CpGs.\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare data\n",
    "train_ids = pd.read_csv(infile_train_ids, header=None).iloc[:, 0].tolist()\n",
    "beta_matrix, clinical_data = load_training_data(train_ids, infile_betavalues, infile_clinical)\n",
    "\n",
    "# convert to M-values\n",
    "mvals = beta2m(beta_matrix, beta_threshold=0.001)\n",
    "infile_cpg_ids = \"./data/set_definitions/CpG_prefiltered_sets/cpg_ids_atac_overlap.txt\"\n",
    "\n",
    "# admin censoring for tnbc\n",
    "# subset methylation atac overlap (needed to match ids)\n",
    "mvals = subset_methylation(mvals,infile_cpg_ids)\n",
    "X = mvals.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d94e60c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Added ['Age', 'Size.mm', 'NHG_1', 'NHG_2', 'NHG_3', 'LN_N+', 'LN_N0'] clinical variables. New X shape: (1008, 193253) ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# onehot encode cat clinvars\n",
    "# subset clinical data aligned to X\n",
    "clin = clinical_data[[\"Age\", \"Size.mm\", \"NHG\", \"LN\"]].loc[X.index]\n",
    "# one-hot encode the categorical clinical variables\n",
    "encoder = OneHotEncoder(drop=None, dtype=float, sparse_output=False)\n",
    "encoded = encoder.fit_transform(clin[[\"NHG\", \"LN\"]])\n",
    "\n",
    "encoded_cols = encoder.get_feature_names_out([\"NHG\", \"LN\"]).tolist()\n",
    "\n",
    "# make a DataFrame for the encoded columns\n",
    "encoded_df = pd.DataFrame(encoded, columns=encoded_cols, index=X.index)\n",
    "# build the encoded clinical DataFrame (drop original categorical cols)\n",
    "clin_encoded = pd.concat([clin.drop(columns=[\"NHG\", \"LN\"]), encoded_df], axis=1)\n",
    "# concatenate encoded clinical back into X\n",
    "X = pd.concat([X, clin_encoded], axis=1).copy()\n",
    "\n",
    "# build clinvars_included_encoded: replace original categorical names with encoded column names\n",
    "clinvars_included_encoded = [c for c in [\"Age\", \"Size.mm\", \"NHG\", \"LN\"] if c not in [\"NHG\", \"LN\"]] + encoded_cols\n",
    "log(f\"Added {clinvars_included_encoded} clinical variables. New X shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ac5f184e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sksurv.util import Surv\n",
    "\n",
    "y = Surv.from_dataframe(\"RFi_event\", \"RFi_years\", clinical_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c95bcd2",
   "metadata": {},
   "source": [
    "## Test example: ERpHER2n_Combined models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "815b85c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(results, checkpoint_file='permutation_checkpoint.pkl'):\n",
    "    \"\"\"Save results after each fold.\"\"\"\n",
    "    with open(checkpoint_file, 'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "    print(f\"Checkpoint saved to {checkpoint_file}\")\n",
    "\n",
    "def load_checkpoint(checkpoint_file='permutation_checkpoint.pkl'):\n",
    "    \"\"\"Load existing results if available.\"\"\"\n",
    "    try:\n",
    "        with open(checkpoint_file, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0d4d5a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_dict = {k: v[0:1] for k, v in outerfolds.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f72e7ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current run for : ERpHER2n_Clinical\n",
      "0\n",
      "Current run for : ERpHER2n_Combined\n",
      "0\n",
      "Current run for : ERpHER2n_Methylation\n",
      "0\n",
      "Current run for : TNBC_Clinical\n",
      "0\n",
      "Current run for : TNBC_Combined\n",
      "0\n",
      "Current run for : TNBC_Methylation\n",
      "0\n",
      "Current run for : All_Clinical\n",
      "0\n",
      "Current run for : All_Combined\n",
      "0\n",
      "Current run for : All_Methylation\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for subcohort_modeltype in small_dict.keys():#outerfolds.keys():\n",
    "    print(f\"Current run for : {subcohort_modeltype}\")\n",
    "    subcohort_modeltype_dict = small_dict[subcohort_modeltype]#outerfolds[subcohort_modeltype]\n",
    "    for entry in subcohort_modeltype_dict:\n",
    "        fold = entry[\"fold\"]\n",
    "        print(fold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8d1ea786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PERMUTATION IMPORTANCE ANALYSIS\n",
      "Started: 2025-12-08 13:44:03\n",
      "Configuration:\n",
      "  - Folds: 10\n",
      "  - Features per fold: ~5,000\n",
      "  - Trees per model: 800\n",
      "  - n_repeats: 3\n",
      "  - max_samples: 0.5\n",
      "Estimated total time: 8-12 hours\n",
      "================================================================================\n",
      "\n",
      "Current run for : ERpHER2n_Clinical\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "   Fold 0/0\n",
      "   Features: 7\n",
      "   Test samples: 101\n",
      "   Started: 13:44:03\n",
      "   Estimated duration: 0-0 minutes\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Checkpoint saved to permutation_checkpoint.pkl\n",
      "   Completed in 0.5 minutes\n",
      "   Top 5 features:\n",
      "      Age: 0.2833\n",
      "      NHG_3: 0.1606\n",
      "      NHG_1: 0.1503\n",
      "      NHG_2: 0.1421\n",
      "      LN_N0: 0.1156\n",
      "\n",
      "   Progress: 1/10 folds complete\n",
      "   Average time per fold: 0.5 minutes\n",
      "   Estimated time remaining: 4 minutes (0.1 hours)\n",
      "Current run for : ERpHER2n_Combined\n",
      "Fold 0: Already computed, skipping...\n",
      "Current run for : ERpHER2n_Methylation\n",
      "Fold 0: Already computed, skipping...\n",
      "Current run for : TNBC_Clinical\n",
      "Fold 0: Already computed, skipping...\n",
      "Current run for : TNBC_Combined\n",
      "Fold 0: Already computed, skipping...\n",
      "Current run for : TNBC_Methylation\n",
      "Fold 0: Already computed, skipping...\n",
      "Current run for : All_Clinical\n",
      "Fold 0: Already computed, skipping...\n",
      "Current run for : All_Combined\n",
      "Fold 0: Already computed, skipping...\n",
      "Current run for : All_Methylation\n",
      "Fold 0: Already computed, skipping...\n",
      "\n",
      "================================================================================\n",
      "ALL FOLDS COMPLETE!\n",
      "Finished: 2025-12-08 13:44:32\n",
      "================================================================================\n",
      "\n",
      "Summary of computation times:\n",
      "  Fold 0: 0.5 minutes\n",
      "\n",
      "Total computation time: 0.5 minutes (0.0 hours)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Main execution\n",
    "results = load_checkpoint()  # Resume if interrupted\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"PERMUTATION IMPORTANCE ANALYSIS\")\n",
    "print(f\"Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  - Folds: 10\")\n",
    "print(f\"  - Features per fold: ~5,000\")\n",
    "print(f\"  - Trees per model: 800\")\n",
    "print(f\"  - n_repeats: 3\")\n",
    "print(f\"  - max_samples: 0.5\")\n",
    "print(f\"Estimated total time: 8-12 hours\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "for subcohort_modeltype in small_dict.keys():#outerfolds.keys():\n",
    "    print(f\"Current run for : {subcohort_modeltype}\")\n",
    "    subcohort_modeltype_dict = small_dict[subcohort_modeltype]#outerfolds[subcohort_modeltype]\n",
    "    for entry in subcohort_modeltype_dict:\n",
    "        fold = entry[\"fold\"]\n",
    "        \n",
    "        # Skip if already computed\n",
    "        if fold in results:\n",
    "            print(f\"Fold {fold}: Already computed, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        if entry[\"model\"] is None:\n",
    "            print(f\"Fold {fold}: No model, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        model = entry[\"model\"]\n",
    "        test_idx = entry[\"test_idx\"]\n",
    "        train_idx = entry[\"train_idx\"]\n",
    "        \n",
    "        features_to_use = entry.get(\"features_after_filter2\") or entry.get(\"features_after_filter1\")\n",
    "        X_test = X.iloc[test_idx][features_to_use]\n",
    "        y_test = y[test_idx]\n",
    "        \n",
    "        print(f\"\\n{'─'*80}\")\n",
    "        print(f\"   Fold {fold}/{len(subcohort_modeltype_dict)-1}\")\n",
    "        print(f\"   Features: {len(features_to_use):,}\")\n",
    "        print(f\"   Test samples: {len(X_test)}\")\n",
    "        print(f\"   Started: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "        \n",
    "        # Estimate time\n",
    "        est_time_min = (len(features_to_use) * 3 * 0.4) / 60  # Conservative estimate\n",
    "        print(f\"   Estimated duration: {est_time_min:.0f}-{est_time_min*1.5:.0f} minutes\")\n",
    "        print(f\"{'─'*80}\")\n",
    "        \n",
    "        try:\n",
    "            # Compute permutation importance\n",
    "            start_time = datetime.now()\n",
    "            perm_result = permutation_importance(\n",
    "                model, \n",
    "                X_test, \n",
    "                y_test, \n",
    "                n_repeats=3,\n",
    "                random_state=42, \n",
    "                n_jobs=-1,\n",
    "                max_samples=0.5\n",
    "            )\n",
    "            \n",
    "            elapsed = (datetime.now() - start_time).total_seconds() / 60\n",
    "            \n",
    "            # Store results\n",
    "            results[fold] = {\n",
    "                'importances_mean': perm_result.importances_mean,\n",
    "                'importances_std': perm_result.importances_std,\n",
    "                'features': features_to_use,\n",
    "                'elapsed_minutes': elapsed,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            # Save checkpoint after each fold\n",
    "            save_checkpoint(results)\n",
    "            \n",
    "            print(f\"   Completed in {elapsed:.1f} minutes\")\n",
    "            \n",
    "            # Show top 5 features\n",
    "            top_idx = np.argsort(perm_result.importances_mean)[-5:][::-1]\n",
    "            print(f\"   Top 5 features:\")\n",
    "            for idx in top_idx:\n",
    "                print(f\"      {features_to_use[idx]}: {perm_result.importances_mean[idx]:.4f}\")\n",
    "            \n",
    "            # Progress update\n",
    "            completed = len(results)\n",
    "            remaining = 10 - completed\n",
    "            avg_time = np.mean([results[f]['elapsed_minutes'] for f in results])\n",
    "            est_remaining = remaining * avg_time\n",
    "            \n",
    "            print(f\"\\n   Progress: {completed}/10 folds complete\")\n",
    "            print(f\"   Average time per fold: {avg_time:.1f} minutes\")\n",
    "            print(f\"   Estimated time remaining: {est_remaining:.0f} minutes ({est_remaining/60:.1f} hours)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   Error in fold {fold}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "\n",
    "# Final save\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"ALL FOLDS COMPLETE!\")\n",
    "print(f\"Finished: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Save final results\n",
    "with open('permutation_importance_final.pkl', 'wb') as f:\n",
    "    pickle.dump(results, f)\n",
    "\n",
    "# Quick summary\n",
    "print(\"Summary of computation times:\")\n",
    "for fold in sorted(results.keys()):\n",
    "    print(f\"  Fold {fold}: {results[fold]['elapsed_minutes']:.1f} minutes\")\n",
    "\n",
    "total_time = sum(results[fold]['elapsed_minutes'] for fold in results)\n",
    "print(f\"\\nTotal computation time: {total_time:.1f} minutes ({total_time/60:.1f} hours)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1052910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERpHER2n_Clinical\n",
      "10\n",
      "ERpHER2n_Combined\n",
      "2\n",
      "ERpHER2n_Methylation\n",
      "2\n",
      "TNBC_Clinical\n",
      "2\n",
      "TNBC_Combined\n",
      "2\n",
      "TNBC_Methylation\n",
      "2\n",
      "All_Clinical\n",
      "2\n",
      "All_Combined\n",
      "2\n",
      "All_Methylation\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for subcohort_modeltype in outerfolds.keys():\n",
    "    print(subcohort_modeltype)\n",
    "    print(len(outerfolds[subcohort_modeltype]))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e977df53",
   "metadata": {},
   "outputs": [],
   "source": [
    "ERpHER2n_Combined_OuterDicts = outerfolds['ERpHER2n_Combined']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "288da985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 trees: C-index = 0.6398\n",
      "800 trees: C-index = 0.6186\n",
      "1000 trees: C-index = 0.6294\n",
      "1500 trees: C-index = 0.6304\n"
     ]
    }
   ],
   "source": [
    "# Quick test on one fold\n",
    "fold_0 = ERpHER2n_Combined_OuterDicts[0]\n",
    "features = fold_0.get(\"features_after_filter2\") or fold_0.get(\"features_after_filter1\")\n",
    "X_test = X.iloc[fold_0[\"test_idx\"]][features]\n",
    "y_test = y[fold_0[\"test_idx\"]]\n",
    "\n",
    "# Test different tree counts\n",
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "\n",
    "for n_trees in [400, 800, 1000, 1500]:\n",
    "    # Get best params\n",
    "    best_params = {\n",
    "        k.replace(\"estimator__randomsurvivalforest__\", \"\"): v \n",
    "        for k, v in fold_0[\"cv_results\"][\"params\"][fold_0[\"cv_results\"][\"rank_test_score\"].argmin()].items()\n",
    "        if k.startswith(\"estimator__\")\n",
    "    }\n",
    "    best_params['n_estimators'] = n_trees\n",
    "    best_params['n_jobs'] = -1\n",
    "    \n",
    "    # Train and test\n",
    "    rsf = RandomSurvivalForest(**best_params)\n",
    "    X_train = X.iloc[fold_0[\"train_idx\"]][features]\n",
    "    y_train = y[fold_0[\"train_idx\"]]\n",
    "    \n",
    "    # Transform with preprocessing\n",
    "    X_train_trans = fold_0[\"model\"].named_steps[list(fold_0[\"model\"].named_steps.keys())[0]].transform(X_train)\n",
    "    X_test_trans = fold_0[\"model\"].named_steps[list(fold_0[\"model\"].named_steps.keys())[0]].transform(X_test)\n",
    "    \n",
    "    rsf.fit(X_train_trans, y_train)\n",
    "    score = rsf.score(X_test_trans, y_test)\n",
    "    \n",
    "    print(f\"{n_trees} trees: C-index = {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ae9376",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ERpHER2n_Combined_OuterDicts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Check actual feature usage in your current models\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m fold_0 \u001b[38;5;241m=\u001b[39m \u001b[43mERpHER2n_Combined_OuterDicts\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      3\u001b[0m rsf \u001b[38;5;241m=\u001b[39m fold_0[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mnamed_steps[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandomsurvivalforest\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m n_trees \u001b[38;5;241m=\u001b[39m rsf\u001b[38;5;241m.\u001b[39mn_estimators\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ERpHER2n_Combined_OuterDicts' is not defined"
     ]
    }
   ],
   "source": [
    "# Check actual feature usage in your current models\n",
    "fold_0 = ERpHER2n_Combined_OuterDicts[0]\n",
    "rsf = fold_0[\"model\"].named_steps['randomsurvivalforest']\n",
    "\n",
    "n_trees = rsf.n_estimators\n",
    "n_features_total = len(fold_0.get(\"features_after_filter2\") or fold_0.get(\"features_after_filter1\"))\n",
    "max_features = rsf.max_features\n",
    "\n",
    "if max_features == \"sqrt\":\n",
    "    max_features_per_tree = int(np.sqrt(n_features_total))\n",
    "elif isinstance(max_features, float):\n",
    "    max_features_per_tree = int(max_features * n_features_total)\n",
    "else:\n",
    "    max_features_per_tree = max_features\n",
    "\n",
    "expected_appearances = n_trees * (max_features_per_tree / n_features_total)\n",
    "\n",
    "print(f\"Total features: {n_features_total}\")\n",
    "print(f\"Features per tree: {max_features_per_tree}\")\n",
    "print(f\"Number of trees: {n_trees}\")\n",
    "print(f\"Expected appearances per feature: {expected_appearances:.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23079445",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = permutation_importance(outerfolds['ERpHER2n_Combined'][0]['model'], X_test, y_test, n_repeats=1, random_state=42)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predict_recurrence",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
